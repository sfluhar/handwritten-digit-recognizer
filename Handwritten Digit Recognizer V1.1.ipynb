{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the exercises in the https://www.datacamp.com/courses/deep-learning-in-python course and the https://www.kaggle.com/c/digit-recognizer competition. The purpose is to correctly classify each of the images, which represent handwritten digits, as as an integer between 0 and 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data and Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines load the packages and libraries that the rest of the code will use. Among these are Keras, a deep learning library for Python users, which runs on top of TensorFlow, a deep learning package developed by Google. I had to download and install both of these before my code would run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train1 = pd.read_csv(\"train1.csv\")\n",
    "train2 = pd.read_csv(\"train2.csv\")\n",
    "train3 = pd.read_csv(\"train3.csv\")\n",
    "train4 = pd.read_csv(\"train4.csv\")\n",
    "\n",
    "test1 = pd.read_csv(\"test1.csv\")\n",
    "test2 = pd.read_csv(\"test2.csv\")\n",
    "test3 = pd.read_csv(\"test3.csv\")\n",
    "\n",
    "train = train1.append(train2)\n",
    "train = train.append(train3)\n",
    "train = train.append(train4)\n",
    "\n",
    "test = test1.append(test2)\n",
    "test = test.append(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 786 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  \\\n",
       "0           0      1       0       0       0       0       0       0       0   \n",
       "1           1      0       0       0       0       0       0       0       0   \n",
       "2           2      1       0       0       0       0       0       0       0   \n",
       "3           3      4       0       0       0       0       0       0       0   \n",
       "4           4      0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel7    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 786 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview the train data, which represents the 784 pixels in each 28-pixel by 28-pixel image\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 786)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the number of rows and columns in the dataframe\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate for Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate the predictors and the labels at this point. This allow us to process the pixels separarely before we fit the model to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include all of the data except the target/label column in X_train\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1)\n",
    "Y_train = train[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values 0 through 9 are distributed fairly evenly through the target. This means the data is balanced, which will make it easier for the classification model to do its job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "g = sns.countplot(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process makes it easier for us to train the deep learning model. There are 255 values for grayscale images, with 0 equal to black and 255 equal to white. Dividing each pixel in the images by 255 yields a number between 0 and 1, so that none of the features in the data will start out with too much influence upon the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "test = test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reshaping is required so that the model can visualize the images in more than one dimension. This allows the model to detect various patterns in the pictures, such as the lines and curves that show up in certain numbers. Keras requires an additional dimension for channels, but we only need one chanel because our images have been converted to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code uses a method from Keras to transform each digit in the target column into ones and zeroes spread over 10 new target columns. For instance, if the digit is 2, the 10 target columns will contain the following vector: [0,0,1,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels to one hot vectors\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process splits the data into training and validation. This split allow us to find the sweetspot for our model. In other words, it helps us to avoid underfitting and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random_seed = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and the validation set for the fitting\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code randomly selects a row from the training data. It convert the vector of 784 pixels into a 28-pixel by 28-pixel square photo. It displays the image in grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQMAAAEBCAYAAAB8GcDAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAECxJREFUeJzt3XFIVff/x/HX/WWWa/sSwb35j7lqstgiE4KslRJMq+nNBcI00YlE+6OEQiYlrsYgEykake0vaQOD6WKOlDSDmqMUho4ZNiaxshaa3WhYNru7es/vj53d79ct77Xrvfdc1/Pxl8eP1/PmgM977r3nem2GYRgC8ML7P6sHABAdiAEAScQAgIkYAJBEDACYiAEAScQAgIkYAJBEDACYiAEAScQAgCkm0jt8+vSp+vr6ZLfbNWfOnEjvHvjXm5iYkMvl0sqVKzV//vxp325GMWhubtZnn32m8fFxvf/++yooKAh4m76+vmn9HICZOXPmjNasWTPtnw86BsPDwzp+/Li+/vprxcbGKi8vT2vXrtVrr73m93Z2u12SdOfOHY2Pjwe7ewBTiImJ0ZIlS3x/a9O+XbA77OzsVGpqqhYuXChJ2rx5s9ra2rRnzx6/t/vrocH4+DgxAMLoeR+GB/0E4v379yeVx+FwaHh4ONhfB8BiQcfA6/XKZrP5tg3DmLQNYHYJOgbx8fFyuVy+bZfLJYfDEZKhAERe0DFYv369urq69PDhQ42Njam9vV1paWmhnA1ABAX9BOLixYu1b98+FRUVyePxKDc3V6tWrQrlbAAiaEbXGTidTjmdzlDNAsBCXI4MQBIxAGAiBgAkEQMAJmIAQBIxAGAiBgAkEQMAJmIAQBIxAGAiBgAkEQMAJmIAQBIxAGAiBgAkEQMAJmIAQBIxAGAiBgAkEQMAJmIAQBIxAGAiBgAkEQMAJmIAQBIxAGAiBgAkEQMAJmIAQBIxAGCa0UeyFxYW6uHDh4qJ+fPXfPLJJ0pOTg7JYAAiK+gYGIahgYEBXb582RcDALNX0A8Tbt68KUkqKSnRtm3bVF9fH7KhAERe0Hfpjx490rp16/TRRx/J4/GoqKhIS5cu1VtvvRXK+QBESNAxSElJUUpKim87NzdXHR0dxACYpYJ+mNDd3a2uri7ftmEYPHcAzGJBx+Dx48eqqamR2+3W6OiompqalJGREcrZAERQ0HflmzZtUm9vr9599115vV7t2LFj0sMGALPLjM7r9+7dq71794ZqFgAW4gpEAJKIAQATMQAgiRgAMBEDAJJm+GrCi2jRokVTrn3wwQd+b3v48OFQjzNtNpvN73p7e7vf9f7+/lCOM0lDQ4Pf9aGhIb/rf71PBjPDmQEAScQAgIkYAJBEDACYiAEAScQAgIkYAJDEdQbP7eOPP55ybffu3X5vaxhGiKeZvkD7fvvtt2e0PhOBjpvb7fa7/t133wW973v37vldLy4uDvp3zzacGQCQRAwAmIgBAEnEAICJGACQRAwAmIgBAElcZ/APr776qt/1/Pz8yAwCn3nz5vldn8nndYyPj/td/+KLL/yuX758Oeh9RxvODABIIgYATMQAgCRiAMBEDABIIgYATMQAgCSuM/iHgYEBv+unT5+eci3Q6+HXr18PZqRpS0pKmnJt8+bNYd13IAkJCVOu/ec//4ngJJONjIz4XX+RPpNhWmcGo6Ojys7O1t27dyVJnZ2dcjqdyszM1PHjx8M6IIDICBiD3t5e5efn++4xnz59qoqKCp06dUrnz59XX1+fOjo6wj0ngDALGIPGxkYdOnRIDodDknTt2jUlJiYqISFBMTExcjqdamtrC/ugAMIr4HMGf/98wPv378tut/u2HQ6HhoeHQz8ZgIh67lcTvF7vpA/xNAwj4Id6Aoh+zx2D+Ph4uVwu37bL5fI9hAAwez13DJKTk3Xr1i3dvn1bExMTamlpUVpaWjhmAxBBz32dwbx581RdXa3S0lK53W6lp6dry5Yt4ZgtKpWXl1s9QlA+/PBDS/efl5c35dqZM2fCuu9bt25NuXbs2DG/t719+3aox4la047BpUuXfF+vW7dO586dC8tAAKzB5cgAJBEDACZiAEASMQBgIgYAJPEWZoTIggUL/K6XlpaGbd9er9fv+pEjR6Zcq6urC/U4sxZnBgAkEQMAJmIAQBIxAGAiBgAkEQMAJmIAQBLXGSBEtm3b5nc9NTU1bPs+ePCg33WuJZgezgwASCIGAEzEAIAkYgDARAwASCIGAEzEAIAkrjNAiFRWVobtd//+++9+1y9evBi2fb9IODMAIIkYADARAwCSiAEAEzEAIIkYADARAwCSuM4A01RTU+N3/fXXXw/bvpubm/2ud3d3h23fL5JpnxmMjo4qOztbd+/elSQdOHBAmZmZysnJUU5ODhd+ALPctM4Ment7VVlZqYGBAd/3+vr6VF9fL4fDEa7ZAETQtM4MGhsbdejQId8f/tjYmAYHB1VRUSGn06kTJ04E/IgrANFtWjE4fPiw1qxZ49t+8OCBUlNTVVVVpcbGRnV3d+vs2bNhGxJA+AX1akJCQoJqa2vlcDgUFxenwsJCdXR0hHo2ABEUVAz6+/t14cIF37ZhGIqJ4YUJYDYLKgaGYaiqqkojIyPyeDxqaGhQRkZGqGcDEEFB3Z2vWLFCu3btUn5+vsbHx5WZmans7OxQz4YIevnll/2ub9261e+6zWYL5TiT7Ny5M2y/G//1XDG4dOmS7+uCggIVFBSEfCAA1uByZACSiAEAEzEAIIkYADARAwCSeAvzCyM2NtbvellZmd/1N954I5TjTPLLL7/4XZ+YmAjbvvFfnBkAkEQMAJiIAQBJxACAiRgAkEQMAJiIAQBJXGfwwliwYIHf9YMHD4Z1/x6PZ8q1I0eO+L2t2+0O9Th4Bs4MAEgiBgBMxACAJGIAwEQMAEgiBgBMxACAJK4zeGFs2bLF0v1///33U66dPn06gpNgKpwZAJBEDACYiAEAScQAgIkYAJBEDACYiAEASdO8zuDkyZNqbW2VJKWnp6u8vFydnZ06cuSI3G63tm7dqn379oV1UMxMRUWFpfv/6aefLN0/Agt4ZtDZ2akrV66oqalJ33zzja5fv66WlhZVVFTo1KlTOn/+vPr6+tTR0RGJeQGEScAY2O127d+/X7GxsZo7d66WL1+ugYEBJSYmKiEhQTExMXI6nWpra4vEvADCJGAMkpKStHr1aknSwMCAWltbZbPZZLfbfT/jcDg0PDwcvikBhN20n0C8ceOGSkpKVF5eroSEBNlsNt+aYRiTtgHMPtOKQU9Pj4qLi1VWVqbt27crPj5eLpfLt+5yueRwOMI2JIDwCxiDoaEh7d69W0ePHlVWVpYkKTk5Wbdu3dLt27c1MTGhlpYWpaWlhX1YAOET8KXFuro6ud1uVVdX+76Xl5en6upqlZaWyu12Kz093fK3yELKzc2dci2cH6kuSU+ePPG7/umnn4Z1/5i5gDGorKxUZWXlM9fOnTsX8oEAWIMrEAFIIgYATMQAgCRiAMBEDABIIgYATPyr9H+ROXPmWLbvs2fP+l3/+eefIzQJgsWZAQBJxACAiRgAkEQMAJiIAQBJxACAiRgAkMR1BgiRmzdvWj0CZogzAwCSiAEAEzEAIIkYADARAwCSiAEAEzEAIInrDP5VNmzYYPUImMU4MwAgiRgAMBEDAJKIAQATMQAgiRgAMBEDAJKmeZ3ByZMn1draKklKT09XeXm5Dhw4oJ6eHsXFxUmS9uzZo4yMjPBNioAWLVpk9QiYxQLGoLOzU1euXFFTU5NsNpt27typixcvqq+vT/X19XI4HJGYE0CYBXyYYLfbtX//fsXGxmru3Llavny5BgcHNTg4qIqKCjmdTp04cUJerzcS8wIIk4AxSEpK0urVqyVJAwMDam1t1caNG5Wamqqqqio1Njaqu7s74MdrAYhu034C8caNGyopKVF5ebmWLVum2tpaORwOxcXFqbCwUB0dHeGcE0CYTSsGPT09Ki4uVllZmbZv367+/n5duHDBt24YhmJieM8TMJsFjMHQ0JB2796to0ePKisrS9Kff/xVVVUaGRmRx+NRQ0MDryQAs1zAu/O6ujq53W5VV1f7vpeXl6ddu3YpPz9f4+PjyszMVHZ2dlgHRWDHjh2bcm3p0qV+b7t27Vq/61999ZXf9bq6Or/riH4BY1BZWanKyspnrhUUFIR8IADW4ApEAJKIAQATMQAgiRgAMBEDAJKIAQATlw3+i/zwww9Trq1fvz6Ck2A24swAgCRiAMBEDABIIgYATMQAgCRiAMAU8ZcWJyYm/twx/wwFCIu//rb++lub9u3CMYw/LpdLkrRkyZJI7xp4obhcLiUmJk77522GYRhhnOcfnj59qr6+Ptntds2ZMyeSuwZeCBMTE3K5XFq5cqXmz58/7dtFPAYAohNPIAKQRAwAmIgBAEnEAICJGACQRAwAmIgBAEkWx6C5uVnvvPOOMjMzdebMGStH+YfCwkJlZWUpJydHOTk56u3ttXokjY6OKjs7W3fv3pUkdXZ2yul0KjMzU8ePH4+auQ4cOKDMzEzfsbt48aIlc508eVJZWVnKyspSTU2NpOg5Zs+azfLjZljk3r17xqZNm4zffvvNePLkieF0Oo0bN25YNc4kXq/X2LBhg+HxeKwexefHH380srOzjTfffNP49ddfjbGxMSM9Pd24c+eO4fF4jJKSEuPbb7+1fC7DMIzs7GxjeHg44rP8r6tXrxrvvfee4Xa7jT/++MMoKioympubo+KYPWu29vZ2y4+bZWcGnZ2dSk1N1cKFC/XSSy9p8+bNamtrs2qcSW7evClJKikp0bZt21RfX2/xRFJjY6MOHTokh8MhSbp27ZoSExOVkJCgmJgYOZ1OS47f3+caGxvT4OCgKioq5HQ6deLECXm93ojPZbfbtX//fsXGxmru3Llavny5BgYGouKYPWu2wcFBy4+bZTG4f/++7Ha7b9vhcGh4eNiqcSZ59OiR1q1bp9raWn3++ef68ssvdfXqVUtnOnz4sNasWePbjpbj9/e5Hjx4oNTUVFVVVamxsVHd3d06e/ZsxOdKSkrS6tWrJUkDAwNqbW2VzWaLimP2rNk2btxo+XGzLAZer1c2m823bRjGpG0rpaSkqKamRq+88ooWLVqk3NxcdXR0WD3WJNF6/BISElRbWyuHw6G4uDgVFhZaeuxu3LihkpISlZeXKyEhIaqO2f/OtmzZMsuPm2UxiI+P972dWfrz7ZZ/nWparbu7W11dXb5twzCi7v8vROvx6+/v14ULF3zbVh67np4eFRcXq6ysTNu3b4+qY/b32aLhuFkWg/Xr16urq0sPHz7U2NiY2tvblZaWZtU4kzx+/Fg1NTVyu90aHR1VU1OTMjIyrB5rkuTkZN26dUu3b9/WxMSEWlpaouL4GYahqqoqjYyMyOPxqKGhwZJjNzQ0pN27d+vo0aPKysqSFD3H7FmzRcNxs+zubvHixdq3b5+Kiork8XiUm5urVatWWTXOJJs2bVJvb6/effddeb1e7dixQykpKVaPNcm8efNUXV2t0tJSud1upaena8uWLVaPpRUrVmjXrl3Kz8/X+Pi4MjMzlZ2dHfE56urq5Ha7VV1d7fteXl5eVByzqWaz+rjx/wwASOIKRAAmYgBAEjEAYCIGACQRAwAmYgBAEjEAYCIGACRJ/w9/6ILiUIUcuAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the image for a randomly selected observation in the predictors data\n",
    "num = random.randint(0,42000)\n",
    "sample_image = plt.imshow(X_train[num][:,:,0],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Layers to Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines of code build the model with methods from Keras. The model will contain a convolutional neural network. We add a series of layers (input, hidden, and output) to the model. The Conv2D layers contain a kernal size that specifies the size of the \"window\" that will pass over each image, looking for patterns that can correlated with specific digits. The ReLU activation function is nonlinear because it allows us to keep positive values and replace negative values with zeroes. The MaxPool2D layer finds the highest values within defined spaces. The BatchNormalization layer rescales the weights the model has learned to improve the training time. The Dropout layer provides regularization, and avoids overfitting, by randomly removing a percentage of nodes from the model. The Flatten layer removes all but one of the dimensions of the data. Near the end of the model are a couple of Dense layers, which are characterized by having every node receive input from every node in the prior layer. The final layer is given 10 outputs and a Softmax activation so it can calculate a probability distribution for any given image over the 10 possible digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sterling\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Sterling\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(784, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines of code launch the RMSProp optimizer from Keras. We can specify the initial learning rate, which the model uses to determine the speed at which it will step through the multi-dimensional gradient to find an optimal solution. The rho, epsilon, and decay parameters also help determine the adaptive learning rate. The goal is to find significant valleys in the gradient that the model can descend to minimize its error rate. The loss function selected is ideal for classification projects, where the target values fall within a range of specified values. The accuracy metric allow us to see the accuracy score during the model fitting process. The model must be compiled with most of these parameters we can fit it to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = RMSprop(lr = 0.001, rho = 0.9, epsilon = 1e-08, decay = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This monitor, available from Keras, reduces the learning rate for the model fitting process when the values for validation accuracy or loss hasn't improved for a specified number of epochs. Ideally, each epoch witnesses and increase in accuracy and a decline in loss each epoch. Each epoch is simply a pass through a batch of observations in the image data. Using a patience value of 2 or 3 is generally recommended. The factor determines the percentage by which the learning rate will be reduced during each update. The verbose parameter controls whether the model prints preliminary results during the fitting process. The monitor also has a minimum learning rate, so that the size of the steps toward gradient descent remain greater than or equal to a certain amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 35 \n",
    "batch_size = 1400 \n",
    "steps_per_epoch = X_train.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a method for slightly transforming images, which helps us to avoid overfitting. These movements occur in random amounts, up to certain limits. The rotations are no more than 10 degrees, the zooming is no more than 10 percent, and the shifts (horizontal and vertical) are no more than 10 percent. The idea is to teach the model how to recognize handwritten digits in a wider range of sizes and positions than would otherwise be available in the regular training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=10, zoom_range = 0.1, width_shift_range=0.1, height_shift_range=0.1)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this process, the learning rate controls the rate of gradient descent as the model searches for minimum values. These values help the model to find optimized weights on the paths that connect nodes from adjacent layers. The weights and the input values are combined in a calculation that determines the node values in the first hidden layer, with a similar interaction occuring between weights and nodes for each subsequent layer. The iterations during the fitting process will also print a few values for each epoch: a loss value on the predictors not in validation, an accuracy score for the predictors not in validation, a validation loss on the predictors in validation, and a validation accuracy score for the predictors in validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sterling\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/35\n",
      "27/27 [==============================] - 134s 5s/step - loss: 1.6699 - acc: 0.5080 - val_loss: 0.2667 - val_acc: 0.9110\n",
      "Epoch 2/35\n",
      "27/27 [==============================] - 130s 5s/step - loss: 0.4223 - acc: 0.8687 - val_loss: 0.1068 - val_acc: 0.9667\n",
      "Epoch 3/35\n",
      "27/27 [==============================] - 127s 5s/step - loss: 0.2037 - acc: 0.9377 - val_loss: 0.0787 - val_acc: 0.9752\n",
      "Epoch 4/35\n",
      "27/27 [==============================] - 125s 5s/step - loss: 0.1499 - acc: 0.9531 - val_loss: 0.0528 - val_acc: 0.9831\n",
      "Epoch 5/35\n",
      "27/27 [==============================] - 124s 5s/step - loss: 0.1074 - acc: 0.9674 - val_loss: 0.0406 - val_acc: 0.9879\n",
      "Epoch 6/35\n",
      "27/27 [==============================] - 125s 5s/step - loss: 0.0772 - acc: 0.9760 - val_loss: 0.0428 - val_acc: 0.9852\n",
      "Epoch 7/35\n",
      "27/27 [==============================] - 125s 5s/step - loss: 0.0810 - acc: 0.9754 - val_loss: 0.0321 - val_acc: 0.9917\n",
      "Epoch 8/35\n",
      "27/27 [==============================] - 128s 5s/step - loss: 0.0666 - acc: 0.9803 - val_loss: 0.0612 - val_acc: 0.9805\n",
      "Epoch 9/35\n",
      "27/27 [==============================] - 128s 5s/step - loss: 0.0625 - acc: 0.9805 - val_loss: 0.0409 - val_acc: 0.9883\n",
      "Epoch 10/35\n",
      "27/27 [==============================] - 127s 5s/step - loss: 0.0519 - acc: 0.9836 - val_loss: 0.0299 - val_acc: 0.9914\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "Epoch 11/35\n",
      "27/27 [==============================] - 125s 5s/step - loss: 0.0350 - acc: 0.9893 - val_loss: 0.0235 - val_acc: 0.9933\n",
      "Epoch 12/35\n",
      "27/27 [==============================] - 125s 5s/step - loss: 0.0309 - acc: 0.9903 - val_loss: 0.0238 - val_acc: 0.9929\n",
      "Epoch 13/35\n",
      "27/27 [==============================] - 123s 5s/step - loss: 0.0313 - acc: 0.9903 - val_loss: 0.0187 - val_acc: 0.9950\n",
      "Epoch 14/35\n",
      "27/27 [==============================] - 127s 5s/step - loss: 0.0301 - acc: 0.9907 - val_loss: 0.0183 - val_acc: 0.9940\n",
      "Epoch 15/35\n",
      "27/27 [==============================] - 129s 5s/step - loss: 0.0277 - acc: 0.9915 - val_loss: 0.0189 - val_acc: 0.9952\n",
      "Epoch 16/35\n",
      "27/27 [==============================] - 128s 5s/step - loss: 0.0259 - acc: 0.9919 - val_loss: 0.0211 - val_acc: 0.9931\n",
      "Epoch 17/35\n",
      "27/27 [==============================] - 128s 5s/step - loss: 0.0280 - acc: 0.9919 - val_loss: 0.0165 - val_acc: 0.9952\n",
      "Epoch 18/35\n",
      "27/27 [==============================] - 127s 5s/step - loss: 0.0259 - acc: 0.9918 - val_loss: 0.0214 - val_acc: 0.9943\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "Epoch 19/35\n",
      "27/27 [==============================] - 127s 5s/step - loss: 0.0221 - acc: 0.9930 - val_loss: 0.0151 - val_acc: 0.9955\n",
      "Epoch 20/35\n",
      "27/27 [==============================] - 132s 5s/step - loss: 0.0199 - acc: 0.9939 - val_loss: 0.0154 - val_acc: 0.9952\n",
      "Epoch 21/35\n",
      "27/27 [==============================] - 130s 5s/step - loss: 0.0207 - acc: 0.9937 - val_loss: 0.0145 - val_acc: 0.9955\n",
      "Epoch 22/35\n",
      "27/27 [==============================] - 129s 5s/step - loss: 0.0191 - acc: 0.9939 - val_loss: 0.0145 - val_acc: 0.9955\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n",
      "Epoch 23/35\n",
      "27/27 [==============================] - 129s 5s/step - loss: 0.0178 - acc: 0.9945 - val_loss: 0.0150 - val_acc: 0.9957\n",
      "Epoch 24/35\n",
      "27/27 [==============================] - 133s 5s/step - loss: 0.0169 - acc: 0.9948 - val_loss: 0.0142 - val_acc: 0.9960\n",
      "Epoch 25/35\n",
      "27/27 [==============================] - 128s 5s/step - loss: 0.0173 - acc: 0.9945 - val_loss: 0.0153 - val_acc: 0.9955\n",
      "Epoch 26/35\n",
      "27/27 [==============================] - 132s 5s/step - loss: 0.0153 - acc: 0.9956 - val_loss: 0.0138 - val_acc: 0.9955\n",
      "Epoch 27/35\n",
      "27/27 [==============================] - 131s 5s/step - loss: 0.0165 - acc: 0.9946 - val_loss: 0.0150 - val_acc: 0.9955\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.\n",
      "Epoch 28/35\n",
      "27/27 [==============================] - 132s 5s/step - loss: 0.0148 - acc: 0.9955 - val_loss: 0.0138 - val_acc: 0.9962\n",
      "Epoch 29/35\n",
      "27/27 [==============================] - 1193s 44s/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0151 - val_acc: 0.9962\n",
      "Epoch 30/35\n",
      "27/27 [==============================] - 222s 8s/step - loss: 0.0153 - acc: 0.9952 - val_loss: 0.0154 - val_acc: 0.9960\n",
      "Epoch 31/35\n",
      "27/27 [==============================] - 226s 8s/step - loss: 0.0142 - acc: 0.9956 - val_loss: 0.0147 - val_acc: 0.9960\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 4.6875000407453626e-05.\n",
      "Epoch 32/35\n",
      "27/27 [==============================] - 218s 8s/step - loss: 0.0107 - acc: 0.9966 - val_loss: 0.0147 - val_acc: 0.9960\n",
      "Epoch 33/35\n",
      "27/27 [==============================] - 211s 8s/step - loss: 0.0133 - acc: 0.9959 - val_loss: 0.0147 - val_acc: 0.9960\n",
      "Epoch 34/35\n",
      "27/27 [==============================] - 214s 8s/step - loss: 0.0137 - acc: 0.9956 - val_loss: 0.0144 - val_acc: 0.9957\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 2.3437500203726813e-05.\n",
      "Epoch 35/35\n",
      "27/27 [==============================] - 380s 14s/step - loss: 0.0151 - acc: 0.9952 - val_loss: 0.0144 - val_acc: 0.9957\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size), epochs = epochs, \n",
    "                              validation_data = (X_val,Y_val), steps_per_epoch = steps_per_epoch, \n",
    "                              callbacks=[learning_rate_reduction]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Kaggle Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines of code use the model to prepare predictions for each observation in the test data. We then generate index numbers, starting at 1, equal to the number of test observations. Together, these two series are saved, with specific names attached, as a single csv file. These are the specifications required for submission files to the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict results for the test data\n",
    "results = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of rows from the dimensions of the results data, which is based on the rows in the test data\n",
    "rows = results.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of the 10 target columns, select the one with the highest value, which indicates the predicted digit \n",
    "results = np.argmax(results,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert those predictions into a Pandas series titled \"Label\"\n",
    "results = pd.Series(results,name=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine a list of index numbers from 1 to 28,000 with the \"Label\"\n",
    "submission = pd.concat([pd.Series(range(1,rows+1),name = \"ImageId\"),results],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the labels and index numbers into a csv file that can be submitted to Kaggle\n",
    "submission.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The submission file has been saved to the same folder as this program.\n"
     ]
    }
   ],
   "source": [
    "print(\"The submission file has been saved to the same folder as this program.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
